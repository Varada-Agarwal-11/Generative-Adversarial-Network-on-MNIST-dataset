{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "Z6gMMBQqP4ZC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "v4m1YKhyEIJ5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discrimiator(nn.Module):\n",
        "  def __init__(self, img_dim):\n",
        "    super().__init__()\n",
        "    self.disc = nn.Sequential(\n",
        "        nn.Linear(img_dim, 128),\n",
        "        nn.LeakyReLU(0.1),\n",
        "        nn.Linear(128, 1),\n",
        "        nn.Sigmoid(),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.disc(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, z_dim, img_dim):\n",
        "    super().__init__()\n",
        "    self.gen = nn.Sequential(\n",
        "        nn.Linear(z_dim, 256),\n",
        "        nn.LeakyReLU(0.1),\n",
        "        nn.Linear(256, img_dim),\n",
        "        nn.Tanh(), #to make sure the value of pixel values is between -1 and 1 after normalization\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.gen(x)\n",
        "\n",
        "lr = 3e-4\n",
        "z_dim = 64\n",
        "image_dim = 784\n",
        "batch_size = 32\n",
        "num_epochs = 50\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ],
      "metadata": {
        "id": "AsfxxzfuGPpZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disc = Discrimiator(image_dim).to(device)\n",
        "gen = Generator(z_dim, image_dim).to(device)\n",
        "fixed_noise = torch.randn(batch_size, z_dim).to(device)\n",
        "transforms = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])"
      ],
      "metadata": {
        "id": "db7coqJWGWkA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.MNIST(root='dataset/', transform=transforms, download=True)"
      ],
      "metadata": {
        "id": "5-EDiADAogoC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DataLoader.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "CH1qyYn0GbOT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
        "criterion = nn.BCELoss()\n",
        "writer_fake = SummaryWriter(f\"GAN_MNIST/fake\")\n",
        "writer_real = SummaryWriter(f\"GAN_MNIST/real\")\n",
        "step = 0"
      ],
      "metadata": {
        "id": "FHpRkNilY38Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  for batch_idx, (real, _) in enumerate(loader):\n",
        "    real = real.view(-1, 784).to(device)\n",
        "    batch_size = real.shape[0]\n",
        "    noise = torch.randn(batch_size, z_dim).to(device)\n",
        "\n",
        "    #Train Discriminator\n",
        "    fake = gen(noise)\n",
        "    disc_real = disc(real).view(-1)\n",
        "    #Criterion is -w_n[y_n(ln(x_n)) + (1-y_n)ln(1-x_n)], so first we take yn as 1, next 0 becasue, together makes the negative of target function that is to be maximized, so we minimize the loss to maximize the target fn\n",
        "    lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
        "    disc_fake = disc(fake).view(-1)\n",
        "    lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
        "    lossD = (lossD_real + lossD_fake) / 2\n",
        "    disc.zero_grad()\n",
        "    lossD.backward(retain_graph= True)\n",
        "    opt_disc.step()\n",
        "\n",
        "    #Train Generator: here, we want to minimize log(1-D(G(z))) but the gradient loss fucntion does train properly, so instead we maximize log(D(G(z)))\n",
        "    output = disc(fake).view(-1)\n",
        "    lossG = criterion(output, torch.ones_like(output))\n",
        "    gen.zero_grad()\n",
        "    lossG.backward()\n",
        "    opt_gen.step()\n",
        "\n",
        "    if batch_idx == 0:\n",
        "      print(\n",
        "          f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
        "          Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
        "      )\n",
        "\n",
        "      with torch.no_grad():\n",
        "        fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
        "        data = real.reshape(-1, 1, 28, 28)\n",
        "        img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
        "        img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
        "\n",
        "        writer_fake.add_image(\n",
        "          \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
        "        )\n",
        "        writer_real.add_image(\n",
        "          \"Mnist Real Images\", img_grid_real, global_step=step\n",
        "        )\n",
        "        step += 1"
      ],
      "metadata": {
        "id": "PQziEgTLZq3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb0d0844-b573-4a34-b5d1-c5869029d5a6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/50] Batch 0/1875           Loss D: 0.6924, loss G: 0.7461\n",
            "Epoch [1/50] Batch 0/1875           Loss D: 0.3047, loss G: 1.6017\n",
            "Epoch [2/50] Batch 0/1875           Loss D: 0.5663, loss G: 0.8626\n",
            "Epoch [3/50] Batch 0/1875           Loss D: 0.5267, loss G: 1.1182\n",
            "Epoch [4/50] Batch 0/1875           Loss D: 0.8383, loss G: 0.7230\n",
            "Epoch [5/50] Batch 0/1875           Loss D: 0.6173, loss G: 1.0907\n",
            "Epoch [6/50] Batch 0/1875           Loss D: 0.9232, loss G: 0.6570\n",
            "Epoch [7/50] Batch 0/1875           Loss D: 0.7700, loss G: 0.7874\n",
            "Epoch [8/50] Batch 0/1875           Loss D: 0.5480, loss G: 1.3472\n",
            "Epoch [9/50] Batch 0/1875           Loss D: 0.9052, loss G: 0.7682\n",
            "Epoch [10/50] Batch 0/1875           Loss D: 0.7633, loss G: 0.9711\n",
            "Epoch [11/50] Batch 0/1875           Loss D: 0.4703, loss G: 1.1517\n",
            "Epoch [12/50] Batch 0/1875           Loss D: 0.5269, loss G: 1.1476\n",
            "Epoch [13/50] Batch 0/1875           Loss D: 0.5305, loss G: 1.1745\n",
            "Epoch [14/50] Batch 0/1875           Loss D: 0.7415, loss G: 0.9321\n",
            "Epoch [15/50] Batch 0/1875           Loss D: 0.4407, loss G: 1.2316\n",
            "Epoch [16/50] Batch 0/1875           Loss D: 0.8298, loss G: 0.8008\n",
            "Epoch [17/50] Batch 0/1875           Loss D: 0.7522, loss G: 0.9160\n",
            "Epoch [18/50] Batch 0/1875           Loss D: 0.7807, loss G: 1.0060\n",
            "Epoch [19/50] Batch 0/1875           Loss D: 0.5810, loss G: 1.1403\n",
            "Epoch [20/50] Batch 0/1875           Loss D: 0.5872, loss G: 1.0524\n",
            "Epoch [21/50] Batch 0/1875           Loss D: 0.6699, loss G: 1.2443\n",
            "Epoch [22/50] Batch 0/1875           Loss D: 0.9368, loss G: 0.5336\n",
            "Epoch [23/50] Batch 0/1875           Loss D: 0.6894, loss G: 1.0806\n",
            "Epoch [24/50] Batch 0/1875           Loss D: 0.6740, loss G: 1.0647\n",
            "Epoch [25/50] Batch 0/1875           Loss D: 0.5586, loss G: 1.1906\n",
            "Epoch [26/50] Batch 0/1875           Loss D: 0.6288, loss G: 1.2755\n",
            "Epoch [27/50] Batch 0/1875           Loss D: 0.6128, loss G: 1.2708\n",
            "Epoch [28/50] Batch 0/1875           Loss D: 0.5561, loss G: 1.1150\n",
            "Epoch [29/50] Batch 0/1875           Loss D: 0.6309, loss G: 1.0863\n",
            "Epoch [30/50] Batch 0/1875           Loss D: 0.7284, loss G: 0.7742\n",
            "Epoch [31/50] Batch 0/1875           Loss D: 0.6895, loss G: 0.8197\n",
            "Epoch [32/50] Batch 0/1875           Loss D: 0.6801, loss G: 0.9647\n",
            "Epoch [33/50] Batch 0/1875           Loss D: 0.6647, loss G: 0.8946\n",
            "Epoch [34/50] Batch 0/1875           Loss D: 0.6690, loss G: 1.0102\n",
            "Epoch [35/50] Batch 0/1875           Loss D: 0.6543, loss G: 1.0005\n",
            "Epoch [36/50] Batch 0/1875           Loss D: 0.6758, loss G: 1.0233\n",
            "Epoch [37/50] Batch 0/1875           Loss D: 0.6729, loss G: 1.0269\n",
            "Epoch [38/50] Batch 0/1875           Loss D: 0.6293, loss G: 0.9891\n",
            "Epoch [39/50] Batch 0/1875           Loss D: 0.7727, loss G: 0.8032\n",
            "Epoch [40/50] Batch 0/1875           Loss D: 0.6435, loss G: 1.0350\n",
            "Epoch [41/50] Batch 0/1875           Loss D: 0.5091, loss G: 1.1587\n",
            "Epoch [42/50] Batch 0/1875           Loss D: 0.6822, loss G: 1.0904\n",
            "Epoch [43/50] Batch 0/1875           Loss D: 0.6143, loss G: 0.9128\n",
            "Epoch [44/50] Batch 0/1875           Loss D: 0.6581, loss G: 0.9580\n",
            "Epoch [45/50] Batch 0/1875           Loss D: 0.5865, loss G: 0.9468\n",
            "Epoch [46/50] Batch 0/1875           Loss D: 0.6590, loss G: 0.8518\n",
            "Epoch [47/50] Batch 0/1875           Loss D: 0.6738, loss G: 0.9509\n",
            "Epoch [48/50] Batch 0/1875           Loss D: 0.5673, loss G: 1.0385\n",
            "Epoch [49/50] Batch 0/1875           Loss D: 0.5523, loss G: 1.1268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(disc.state_dict(), \"discriminator.pth\")\n",
        "torch.save(gen.state_dict(), \"generator.pth\")\n"
      ],
      "metadata": {
        "id": "W28epcJIZXSf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Or0p-u1B0WA4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}